{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbeerProg/RRDS/blob/main/Augmentation%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFsQ-z5st7HD",
        "outputId": "b3b92133-1b78-4f0c-97fb-d26a31d71ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L56a_di-uGow",
        "outputId": "77b9d960-aa59-41e3-fa17-73f3c27c8825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "import re\n",
        "import multiprocessing\n",
        "from nltk.corpus import wordnet\n",
        "from deep_translator import GoogleTranslator\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# ------------------------------\n",
        "# Define Text Augmentation Functions\n",
        "# ------------------------------\n",
        "\n",
        "def synonym_replacement(text, n=2):\n",
        "    \"\"\"Replace up to n words in the text with their synonyms using WordNet.\"\"\"\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set(words))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = wordnet.synsets(random_word)\n",
        "        if synonyms:\n",
        "            # Use the first synonym found\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def random_deletion(text, p=0.2):\n",
        "    \"\"\"Randomly delete words from the text with probability p.\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) == 1:\n",
        "        return text\n",
        "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
        "    return \" \".join(new_words) if new_words else text\n",
        "\n",
        "def swap_words(text, n=2):\n",
        "    \"\"\"Randomly swap two words in the text n times.\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < 2:\n",
        "        return text\n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def back_translate(text, src=\"en\", target=\"fr\"):\n",
        "    \"\"\"Perform back translation: English -> French -> English.\"\"\"\n",
        "    try:\n",
        "        translated = GoogleTranslator(source=src, target=target).translate(text)\n",
        "        back_translated = GoogleTranslator(source=target, target=src).translate(translated)\n",
        "        return back_translated\n",
        "    except Exception as e:\n",
        "        return text\n",
        "\n",
        "def parallel_back_translate(text_list, num_workers=4):\n",
        "    \"\"\"Uses multiprocessing to speed up back translation.\"\"\"\n",
        "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
        "        results = list(tqdm(pool.imap(back_translate, text_list), total=len(text_list)))\n",
        "    return results\n",
        "\n",
        "# ------------------------------\n",
        "# Define Numeric Augmentation Function\n",
        "# ------------------------------\n",
        "\n",
        "def augment_numeric_features(row, numeric_cols, noise_factor=0.05):\n",
        "    \"\"\"\n",
        "    For each numeric column, add a small random noise\n",
        "    equal to ±(noise_factor * value).\n",
        "    \"\"\"\n",
        "    new_values = []\n",
        "    for col in numeric_cols:\n",
        "        try:\n",
        "            val = float(row[col])\n",
        "        except:\n",
        "            val = 0\n",
        "        noise = random.uniform(-noise_factor, noise_factor) * val\n",
        "        new_values.append(val + noise)\n",
        "    return new_values\n",
        "\n",
        "# ------------------------------\n",
        "# Load Original Dataset\n",
        "# ------------------------------\n",
        "\n",
        "df = pd.read_excel(\"Final_dataset.xlsx\")\n",
        "df.columns = df.columns.str.strip()  # Clean column names\n",
        "\n",
        "all_columns = df.columns.tolist()\n",
        "numeric_cols = all_columns[1:-1]  # All columns between text and label\n",
        "\n",
        "print(\"Original columns:\", all_columns)\n",
        "print(\"Numeric columns:\", numeric_cols)\n",
        "\n",
        "# ------------------------------\n",
        "# Parallel Back Translation for Speed\n",
        "# ------------------------------\n",
        "\n",
        "print(\"Performing parallel back translation on text column...\")\n",
        "back_translated_texts = parallel_back_translate(df[\"text\"].tolist(), num_workers=4)\n",
        "\n",
        "df[\"back_translation\"] = back_translated_texts\n",
        "print(\"Back translation complete.\")\n",
        "\n",
        "# ------------------------------\n",
        "# Create Augmented Data (New File)\n",
        "# ------------------------------\n",
        "\n",
        "# We'll create a new DataFrame with augmented versions of the text.\n",
        "# Each augmented row will have the same columns as the original (excluding the back_translation column).\n",
        "# For each original row, we'll generate 4 augmented rows:\n",
        "# 1. Synonym Replacement\n",
        "# 2. Random Deletion\n",
        "# 3. Word Swap\n",
        "# 4. Back Translation (from our parallel output)\n",
        "\n",
        "# Exclude 'back_translation' from column list for final augmented file.\n",
        "aug_columns = [col for col in all_columns if col != \"back_translation\"]\n",
        "\n",
        "augmented_rows = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    original_text = row[\"text\"]      # The original text is still available if needed\n",
        "    back_text = row[\"back_translation\"]  # Use our back-translated version\n",
        "    label = row[\"label\"]\n",
        "\n",
        "    # Apply text augmentation techniques:\n",
        "    aug_text_syn = synonym_replacement(original_text)\n",
        "    aug_text_del = random_deletion(original_text)\n",
        "    aug_text_swap = swap_words(original_text)\n",
        "    aug_text_back = back_text  # Already processed via parallel back translation\n",
        "\n",
        "    # Augment numeric features (for each augmentation we add random noise)\n",
        "    aug_numeric_syn = augment_numeric_features(row, numeric_cols)\n",
        "    aug_numeric_del = augment_numeric_features(row, numeric_cols)\n",
        "    aug_numeric_swap = augment_numeric_features(row, numeric_cols)\n",
        "    aug_numeric_back = augment_numeric_features(row, numeric_cols)\n",
        "\n",
        "    # Build augmented row (structure: [text] + [numeric features] + [label])\n",
        "    row_syn = [aug_text_syn] + aug_numeric_syn + [label]\n",
        "    row_del = [aug_text_del] + aug_numeric_del + [label]\n",
        "    row_swap = [aug_text_swap] + aug_numeric_swap + [label]\n",
        "    row_back = [aug_text_back] + aug_numeric_back + [label]\n",
        "\n",
        "    augmented_rows.append(row_syn)\n",
        "    augmented_rows.append(row_del)\n",
        "    augmented_rows.append(row_swap)\n",
        "    augmented_rows.append(row_back)\n",
        "\n",
        "# Debug: Check sample augmented row length\n",
        "print(\"Sample augmented row length:\", len(augmented_rows[0]))\n",
        "print(\"Expected number of columns:\", len(aug_columns))\n",
        "\n",
        "# Create a new DataFrame for augmented data\n",
        "augmented_df = pd.DataFrame(augmented_rows, columns=aug_columns)\n",
        "\n",
        "augmented_df.to_excel(\"augmented_dataset.xlsx\", index=False)\n",
        "print(\"Augmented dataset saved as 'augmented_dataset.xlsx'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4es4vO6LvpB",
        "outputId": "e022814d-377d-4a41-820b-aa56ec2e95e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original columns: ['text', 'PM Ratio', 'FPSP Ratio', 'Review Length', 'RW Ratio', 'Sentiment', 'Generalization', 'Passive Voice', 'Total reviewer reviews', 'Account type', 'Useful votes', 'Attached Medias', 'label']\n",
            "Numeric columns: ['PM Ratio', 'FPSP Ratio', 'Review Length', 'RW Ratio', 'Sentiment', 'Generalization', 'Passive Voice', 'Total reviewer reviews', 'Account type', 'Useful votes', 'Attached Medias']\n",
            "Performing parallel back translation on text column...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21476/21476 [34:35<00:00, 10.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Back translation complete.\n",
            "Sample augmented row length: 13\n",
            "Expected number of columns: 13\n",
            "Augmented dataset saved as 'augmented_dataset.xlsx'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMKR7MVlUmHgqXO2vXhYGUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}